import json
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
import gensim
from gensim import corpora
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from gensim.models import CoherenceModel
import webbrowser

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Initialize lemmatizer and stop words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Step 1: Load Dataset from JSON
with open('your_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Check the type of the data and print out its structure
print("Type of data:", type(data))
print("Keys in the data:", data.keys())

# Print a sample of the data to understand its structure
# This will help you see how the text is structured inside the JSON
sample_data = data if isinstance(data, dict) else data[:3]  # Print first 3 items if it's a list
print("\nSample Data Preview:", sample_data)

# Step 2: Extract Documents (Update based on the actual structure)
content_field = data.get('content', None)  # Look for the 'content' field or update if necessary

# Check the type and structure of the 'content' field
print("Content field type:", type(content_field))

if isinstance(content_field, dict):
    # If it's a dictionary, process its values
    print("Keys in the content field:", content_field.keys())
    documents = [content_field[key] for key in content_field if isinstance(content_field[key], str)]
elif isinstance(content_field, list):
    # If it's a list, assume each item in the list is a document
    documents = [item for item in content_field if isinstance(item, str)]
else:
    # If it's something else, print an error message
    print("Content field is neither a dictionary nor a list. Unable to process.")
    documents = []

# Step 3: Check if documents were successfully extracted
if not documents:
    print("No valid documents found! Please check your data extraction logic.")
else:
    # Step 4: Sort the documents alphabetically
    documents.sort()

    # Preview sorted documents (Show only first 10 for preview)
    sorted_documents_preview = "\n".join(documents[:10])  # Adjust number as needed
    print("Sorted documents (Preview):\n", sorted_documents_preview)

    # Step 5: Preprocess the Text Data
    def preprocess(text):
        # 1. Remove non-alphabetic characters (keeping only words)
        text = re.sub(r'\W', ' ', text)
        text = text.lower()  # Convert text to lowercase

        # 2. Tokenize words
        tokens = word_tokenize(text)

        # 3. Remove stop words and words with length <= 2
        tokens = [word for word in tokens if word not in stop_words and len(word) > 2]

        # 4. Lemmatize each word
        tokens = [lemmatizer.lemmatize(word) for word in tokens]

        return tokens

    # Apply preprocessing to each document
    processed_docs = [preprocess(doc) for doc in documents]

    # Step 6: Create Dictionary and Corpus for LDA
    if processed_docs:  # Only proceed if we have some valid processed docs
        dictionary = corpora.Dictionary(processed_docs)
        corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

        # Step 7: Train the LDA Model
        num_topics = 5  # You can adjust the number of topics
        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)

        # Display the topics generated by LDA
        print("\nTopics generated by LDA Model:")
        for idx, topic in lda_model.print_topics(-1):
            print(f"Topic {idx + 1}: {topic}")

        # Step 8: Visualize Topics with pyLDAvis
        lda_vis_data = gensimvis.prepare(lda_model, corpus, dictionary)
        pyLDAvis.save_html(lda_vis_data, 'lda_visualization.html')  # Save visualization to an HTML file
        print("\nVisualization saved as 'lda_visualization.html'. Open it in a browser to explore the topics interactively.")

        # Step 9: Open the HTML file in the browser
        webbrowser.open('lda_visualization.html')

        # Step 10: Evaluate Model Quality with Coherence Score
        coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')
        coherence_score = coherence_model_lda.get_coherence()
        print(f"\nCoherence Score: {coherence_score}")
    else:
        print("No valid documents to process. Please check your data extraction step.")